<!-- 
- [UML클래스전략패턴](https://github.com/hyeonDD/fluent_python/blob/master/Part14/ex14-13/UML_class_diagram.png)
 -->
# 사례 연구: 데이터베이스 변환 유틸리티 안의 제너레이터
몇 년 전 필자는 브라질 상파울루에 있는 전미보건단체/세계보건기구(PAHO/WHO)에서 운영하는 전자 도서관인 BIREME에서 작업하고 있었다. BIREME가 생성한 참고문헌 데이터셋 중에는 라틴아메리카 및 카리브해 보건 인덱스(LILACS)와 과학 온라인 전자 도서관 (SciELO)이라는 브라질에서 만든 과학과 기술 문서를 인덱싱하는 종합 데이터베이스가 있었다.

1980년대 후반 이후 LILACS를 관리하기 위해 사용된 데이터베이스 시스템은 유네스코에서 만든 비관계형 문서 데이터베이스 시스템인 CDS?ISIS를 GNU/리눅스 서버에서 실행하기 위해 BIREME에서 C로 재작성성한 시스템이었다. 필자의 업무 중 하나는 LILACS(그리고 궁극적으로는 훨씬 더 큰 SciELO) 데이터베이스를 카우치DB나 몽고DB 등의 최신 오픈 소스 문서 데이터베이스로 마이그레이션하기 위한 방법을 연가흔 것이었다.

연구의 일환으로 필자는 CDS/ISIS 파일을 읽어서 카우치DB나 몽고DB로 임포트하기 적합한 JSON 파일을 생성하는 isis2json.py 스크립트를 작성했다. 처음에는 스크립트가 CSS/ISIS에서 익스포트된 ISO-2709 포맷의 파일을 읽었다. 전체 데이터셋이 주메모리보다 훨씬 컸기 때문에 읽기와 쓰기는 점진적으로 수행해야 했다. for 루프를 반복하면서 .iso 파일에서 레코드 하나를 읽고, 처리하고, .json 파일에 쓰면 되므로, 이 과정은 상당히 쉬웠다.

그러나 운용 문제 때문에 isis2json.py는 CDS/ISIS의 또 다른 데이터 포맷인 .mst 파일도 지원해야 했다. .mst는 ISO-2709로 익스포트하는 비용을 피하기 위해 BIREME에서 사용하던 이전 파일 포맷이다.

이제 문제가 생겼다. ISO-2709 파일과 .mst 파일을 읽기 위해 사용하는 라이브러리의 API가 너무 달랐다. 그리고 각 출력 레코드의 구조를 변경하기 위해 스크립트가 다양한 명령행 옵션을 받아야 했으므로, JSON을 생성하는 루프도 이미 상당히 복잡해져 있었다. JSON을 생성하는 하나의 for 루프 안에서 두 개의 전혀 다른 API를 이용해서 파일을 읽는 것은 처리하기 힘든 일이다.

필자는 읽는 논리의 한 쌍의 제너레이터 함수에 분할해 넣음으로써 문제를 해결했다. 제너레이터 하나가 하나의 포맷을 읽는 것이다. 결국 isis2json.py 스크립트는 4개의 함수로 나누어졌다. [예제 A-5]에서 파이썬 2로 작성된 핵심 스크립트를 볼 수 있지만, 전체 소스코드는 깃허브의 fluentpython/isis2json (http://bit.ly/1HGqzzT) 에서 볼 수 있다.
스크립트의 구조는 다음과 같이 간략히 정리할 수 있다.

---

**main()**
main() 함수는 argparse()를 이용해서 명령행 옵션을 읽고 출력 레코드의 구조를 설정한다. 입력 파일명의 확장자를 이용해서 적절한 제너레이터 함수를 선택하고, 데이터를 읽어서 레코드를 하나씩 생성한다.

**iter_iso_records()**
이 제너레이터 함수는 ISO-2709 포맷으로 되어 있는 .sio 파일을 읽는다. 이 함수는 파일명과 isis_json_type(레코드 구조와 관련된 옵션 중 하나)을 인수로 받는다. for 루프를 반복할 때마다 레코드를 하나 읽고, 빈 딕셔너리를 생성하고, 데이터를 채워서, 딕셔너리를 생성한다.

**iter_mst_records()**
이 제너레이터 함수는 .mst 파일을 읽는다. isis2json.py 소스 코드를 보면 이 코드는 iter_iso_records 만큼 간단하지는 않지만, 인터페이스와 전체 구조는 동일하다는 것을 알 수 있다. 파일명과 isis_json_type 인수를 받아서 for 루프에 들어가고, 반복할 때마다 레코드 하나에 대응하는 딕셔너리를 하나 생성한다.

**write_json()**
이 함수는 JSON 레코드를 한 번에 하나씩 기록한다. 아주 많은 인수를 받지만, 첫 번째 인수인 input_gen은 iter_iso_records()와 iter_mst_records() 제너레이터 함수 중 하나를 가리킨다. 이 함수의 핵심 for 루프에서는 선택된 input_gen 제너레이터가 생성하는 딕셔너리를 반복하고, 명령행 옵션에 따라 다양하게 처리하고, JSON 레코드를 출력 파일의 뒤에 저장한다.

---

제너레이터 함수를 활용해서 필자는 읽는 논리와 쓰는 논리를 분리시킬 수 있었다. 물론 레코드 모두를 한꺼번에 메모리로 읽고 처리해서 저장하는 방식이 가장 간단한 분리 방법이겠지만, 데이터베이스 크기 때문에 이 방법은 사용할 수 없다. 제너레이터를 사용해서 읽기와 쓰기를 번갈아 수행하므로, 스크립트는 파일 크기에 구애받지 않고 처리할 수 있다.

이제는 isis2json.py가 예를 들어 MARCXML (미국국회도서관에서 ISO-2709 데이터를 표현하기 위해 사용하는 DTD)이라는 입력 포맷을 추가로 지원해야 하는 경우에도, write_json() 함수를 변경할 필요 없이 읽기 논리를 구현하는 세 번째 제너레이터 함수를 쉽게 추가할 수 있다.

이것은 최첨단 기술은 아니지만, 데이터베이스를 레코드 스트림으로 처리하고, 데이터 크기에 상관없이 메모리 사용량을 적게 유지하는 해결책을 제시하는 제너레이터의 실제 사례다. 대용량 데이터셋을 관리하는 사람이라면 누구나 제너레이터를 실무에 활용할 기회가 많을 것이다.

다음 절에서는 제너레이터의 또 다른 측면을 설명한다.

# 코루틴으로서의 제너레이터
파이썬 2.2에서 yield 키워드로 제너레이터 함수를 지원한지 약 5년이 지나서, 파이썬 2.5에 'PEP 342 - 향상된 제너레이터를 이용한 코루틴' 제안서 (https://www.python.org/dev/peps/pep-0342) 가 구현되었다. 이 제안서는 제너레이터 객체에 send() 등의 메서드와 기능을 추가했다.

__next__()와 마찬가지로 send()도 제너레이터가 다음 yield로 넘어가게 만들지만, 제너레이터를 사용하는 호출자가 제너레이터에 데이터를 보낼 수도 있게 해준다. send()에 전달된 인수는 모두 제너레이터 함수 본체 안에서 해당 yield 표현식의 값이 된다. 즉, 호출자가 제너레이터로부터 데이터를 받는 것만 허용하는 __next__()와 달리, send() 메서드는 호출자와 제너레이터가 양방향으로 데이터를 교환할 수 있게 해준다.

이것은 정말 대단한 발전으로서, 실제로 제너레이터의 성질을 바꿔놓았다. 이런 방식으로 사용할때 제너레이터는 **코루틴**이 된다. 파이썬 커뮤니티에서 코루틴에 대해 가장 많이 기고하고 발표한 데이비드 비즐리는 PyCon US 2009 튜토리얼 (http://www.dabeaz.com/coroutines/) 에서 다음과 같이 경고한다.
* 제너레이터는 반복하기 위한 데이터를 생성한다.
* 코루틴은 데이터의 소비자다.
* 머리가 터지지 않으려면 이 두 개념을 뒤섞지 마라.
* 코루틴은 반복과 상관없다.
* 주의: 코루틴 안에서 yield가 값을 생성하게 하는 것은 쓸모가 있지만, 이것은 반복과 상관없다.

데이비드 비즐리의 말에 동감하며 이 장을 마친다. 이 장에서는 반복에 대해서만 설명하고, 제너레이터를 코루틴으로 사용할 수 있게 해주는 send()및 여러 기능에 대해서는 손도 대지 않았다. 코루틴은 16장에서 설명한다.